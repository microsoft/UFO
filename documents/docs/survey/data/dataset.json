[
    {
        "Name": "Mind2Web: Towards a Generalist Agent for the Web",
        "Platform": "Web",
        "Date": "June 2023",
        "Paper_Url": "http://arxiv.org/abs/2306.06070v3",
        "Highlight": "Develops generalist web agents with diverse user interactions on real-world websites",
        "Code_Url": "https://osu-nlp-group.github.io/Mind2Web/"
    },
    {
        "Name": "WebVLN: Vision-and-Language Navigation on Websites",
        "Platform": "Web",
        "Date": "December 2023",
        "Paper_Url": "http://arxiv.org/abs/2312.15820v1",
        "Highlight": "Vision-and-language navigation for human-like web browsing",
        "Code_Url": "https://github.com/WebVLN/WebVLN"
    },
    {
        "Name": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
        "Platform": "Web",
        "Date": "February 2024",
        "Paper_Url": "http://arxiv.org/abs/2402.05930v2",
        "Highlight": "The first large-scale dataset designed to evaluate agents in real-world conversational web navigation",
        "Code_Url": "https://mcgill-nlp.github.io/weblinx/"
    },
    {
        "Name": "VGA: Vision GUI Assistant -- Minimizing Hallucinations through Image-Centric Fine-Tuning",
        "Platform": "Android",
        "Date": "June 2024",
        "Paper_Url": "https://arxiv.org/abs/2406.14056",
        "Highlight": "Prioritizes visual content to reduce inaccuracies",
        "Code_Url": "https://github.com/Linziyang1999/Vision-GUI-Assistant"
    },
    {
        "Name": "Rico: A Mobile App Dataset for Building Data-Driven Design Applications",
        "Platform": "Android",
        "Date": "October 2017",
        "Paper_Url": "https://doi.org/10.1145/3126594.3126651",
        "Highlight": "Comprehensive dataset for mobile UI design, interaction modeling, layout generation",
        "Code_Url": "http://www.interactionmining.org/"
    },
    {
        "Name": "Mapping Natural Language Instructions to Mobile UI Action Sequences",
        "Platform": "Android",
        "Date": "May 2020",
        "Paper_Url": "http://arxiv.org/abs/2005.03776v2",
        "Highlight": "Pioneering method for grounding natural language instructions to executable mobile UI actions",
        "Code_Url": "https://github.com/google-research/google-research/tree/master/seq2act"
    },
    {
        "Name": "A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility",
        "Platform": "Android",
        "Date": "February 2022",
        "Paper_Url": "http://arxiv.org/abs/2202.02312v3",
        "Highlight": "Task feasibility prediction for interactive GUI in mobile apps",
        "Code_Url": "https://github.com/aburns4/MoTIF"
    },
    {
        "Name": "META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI",
        "Platform": "Android",
        "Date": "May 2022",
        "Paper_Url": "http://arxiv.org/abs/2205.11029v2",
        "Highlight": "Task-oriented dialogue system for mobile GUI without relying on back-end APIs",
        "Code_Url": "https://x-lance.github.io/META-GU"
    },
    {
        "Name": "Android in the Wild: A Large-Scale Dataset for Android Device Control",
        "Platform": "Android",
        "Date": "July 2023",
        "Paper_Url": "http://arxiv.org/abs/2307.10088v2",
        "Highlight": "Large-scale dataset for device control research with extensive app and UI diversity",
        "Code_Url": "https://github.com/google-research/google-research/tree/master/android_in_the_wild"
    },
    {
        "Name": "GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices",
        "Platform": "Android",
        "Date": "June 2024",
        "Paper_Url": "http://arxiv.org/abs/2406.08451v1",
        "Highlight": "Focuses on cross-app navigation tasks on mobile devices",
        "Code_Url": "https://github.com/OpenGVLab/GUI-Odyssey"
    },
    {
        "Name": "AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents",
        "Platform": "Android",
        "Date": "July 2024",
        "Paper_Url": "http://arxiv.org/abs/2407.17490v1",
        "Highlight": "Multi-level, large-scale annotations supporting complex mobile GUI tasks",
        "Code_Url": "https://yuxiangchai.github.io/AMEX/"
    },
    {
        "Name": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
        "Platform": "iOS, Android",
        "Date": "April 2024",
        "Paper_Url": "http://arxiv.org/abs/2404.05719v1",
        "Highlight": "Benchmark for UI-centric tasks with adjustable screen aspect ratios",
        "Code_Url": "https://github.com/apple/ml-ferret"
    },
    {
        "Name": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents",
        "Platform": "Android",
        "Date": "March 2024",
        "Paper_Url": "http://arxiv.org/abs/2403.02713v2",
        "Highlight": "Structured \"Chain-of-Action-Thought\" enhancing GUI navigation",
        "Code_Url": "https://github.com/IMNearth/CoAT"
    },
    {
        "Name": "Octo-planner: On-device Language Model for Planner-Action Agents",
        "Platform": "Android",
        "Date": "June 2024",
        "Paper_Url": "http://arxiv.org/abs/2406.18082v1",
        "Highlight": "Optimized for task planning with GUI actions",
        "Code_Url": "https://huggingface.co/NexaAIDev/octopus-planning"
    },
    {
        "Name": "E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion",
        "Platform": "Android tiny-apps",
        "Date": "June 2024",
        "Paper_Url": "http://arxiv.org/abs/2406.14250v3",
        "Highlight": "First large-scale Chinese dataset for GUI navigation with real human interactions",
        "Code_Url": "/"
    },
    {
        "Name": "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding",
        "Platform": "Android",
        "Date": "September 2024",
        "Paper_Url": "http://arxiv.org/abs/2409.14818v2",
        "Highlight": "Large-scale Chinese mobile GUI dataset with unique navigation graph",
        "Code_Url": "https://github.com/Meituan-AutoML/MobileVLM"
    },
    {
        "Name": "AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents",
        "Platform": "Android",
        "Date": "October 2024",
        "Paper_Url": "http://arxiv.org/abs/2410.24024v2",
        "Highlight": "XML-based interaction data with unified action space",
        "Code_Url": "https://github.com/THUDM/Android-Lab"
    },
    {
        "Name": "MobileViews: A Large-Scale Mobile GUI Dataset",
        "Platform": "Android",
        "Date": "September 2024",
        "Paper_Url": "http://arxiv.org/abs/2409.14337v2",
        "Highlight": "Largest open-source mobile screen dataset",
        "Code_Url": "https://huggingface.co/datasets/mllmTeam/MobileViews"
    },
    {
        "Name": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
        "Platform": "Linux, Windows",
        "Date": "February 2024",
        "Paper_Url": "http://arxiv.org/abs/2402.07945v1",
        "Highlight": "VLM-based agent across multiple desktop environments",
        "Code_Url": "https://github.com/niuzaisheng/ScreenAgent"
    },
    {
        "Name": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents",
        "Platform": "Android, Web",
        "Date": "August 2024",
        "Paper_Url": "http://arxiv.org/abs/2408.06327v1",
        "Highlight": "Systematic evaluation of VLM as a visual foundation agent across multiple scenarios",
        "Code_Url": "https://github.com/THUDM/VisualAgentBench"
    },
    {
        "Name": "GUICourse: From General Vision Language Models to Versatile GUI Agents",
        "Platform": "Android, Web",
        "Date": "June 2024",
        "Paper_Url": "http://arxiv.org/abs/2406.11317v1",
        "Highlight": "Dataset suite for enhancing VLM GUI navigation on web and mobile platforms",
        "Code_Url": "https://github.com/yiye3/GUICourse"
    },
    {
        "Name": "GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents",
        "Platform": "Computer, Mobile, Web, XR",
        "Date": "June 2024",
        "Paper_Url": "http://arxiv.org/abs/2406.10819v1",
        "Highlight": "Designed for dynamic, sequential GUI tasks with video data",
        "Code_Url": "https://gui-world.github.io/"
    },
    {
        "Name": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
        "Platform": "Android, iOS, Computer, Web",
        "Date": "February 2024",
        "Paper_Url": "http://arxiv.org/abs/2402.04615v3",
        "Highlight": "Comprehensive pretraining and fine-tuning for GUI tasks across platforms",
        "Code_Url": "https://github.com/google-research-datasets/screen_annotation"
    },
    {
        "Name": "OmniParser for Pure Vision Based GUI Agent",
        "Platform": "Web, Computer, Mobile",
        "Date": "August 2024",
        "Paper_Url": "http://arxiv.org/abs/2408.00203v1",
        "Highlight": "Vision-based parsing of UI screenshots into structured elements",
        "Code_Url": "https://github.com/microsoft/OmniParser"
    },
    {
        "Name": "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents",
        "Platform": "Web, Android",
        "Date": "October 2024",
        "Paper_Url": "http://arxiv.org/abs/2410.05243v1",
        "Highlight": "Largest dataset for GUI visual grounding",
        "Code_Url": "https://osu-nlp-group.github.io/UGround/"
    },
    {
        "Name": "xLAM: A Family of Large Action Models to Empower AI Agent Systems",
        "Platform": "Web and tools used",
        "Date": "September 2024",
        "Paper_Url": "http://arxiv.org/abs/2409.03215v1",
        "Highlight": "Provides a unified format across diverse environments, enhancing generalizability and error detection for GUI agents",
        "Code_Url": "https://github.com/SalesforceAIResearch/xLAM"
    }
]